import numpy as np
import torch
from sentence_transformers import SentenceTransformer
import re
from typing import List, Union


class EmbeddingGenerator:
    """
    G√©n√®re des embeddings en utilisant un mod√®le Sentence Transformers multilingue
    sp√©cialement con√ßu pour la similarit√© s√©mantique.
    """
    
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        Initialise le gestionnaire d'embeddings.
        
        Args:
            model_name (str): Nom du mod√®le Sentence Transformers.
                             Par d√©faut: "paraphrase-multilingual-MiniLM-L12-v2"
        """
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üîß Utilisation du p√©riph√©rique : {self.device}")
        
        try:
            # Chargement du mod√®le Sentence Transformers
            print(f"üì• Chargement du mod√®le {model_name}...")
            self.model = SentenceTransformer(model_name, 
                                             device=str(self.device),
                                             backend="torch")
            
            print("‚úÖ Mod√®le Sentence Transformers charg√© et configur√©.")
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            raise RuntimeError(f"Impossible de charger le mod√®le {model_name}: {str(e)}")
    
    def _clean_text(self, text: str) -> str:
        """
        Nettoie et normalise le texte d'entr√©e.
        
        Args:
            text (str): Texte √† nettoyer
            
        Returns:
            str: Texte nettoy√©
        """
        if not isinstance(text, str):
            text = str(text)
        
        # Suppression des caract√®res sp√©ciaux excessifs
        text = re.sub(r'[^\w\s√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√ß\'-]', ' ', text)
        
        # Normalisation des espaces
        text = re.sub(r'\s+', ' ', text)
        
        # Suppression des espaces en d√©but/fin
        text = text.strip()
        
        return text
    
    def generate_embedding(self, text: str) -> np.ndarray:
        """
        G√©n√®re un embedding pour un texte donn√©.
        
        Args:
            text (str): Texte √† encoder
            
        Returns:
            np.ndarray: Embedding du texte (vecteur de features normalis√©)
        """
        # Nettoyage et pr√©paration du texte
        cleaned_text = self._clean_text(text)
        
        if not cleaned_text.strip():
            raise ValueError("Le texte d'entr√©e est vide apr√®s nettoyage.")
        
        try:
            # G√©n√©ration de l'embedding avec Sentence Transformers
            # Le mod√®le g√®re automatiquement la tokenisation, l'encodage et la normalisation
            embedding = self.model.encode(
                cleaned_text,
                convert_to_numpy=True,
                normalize_embeddings=True  # Normalisation automatique
            )
            
            return embedding
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de l'embedding: {e}")
            raise RuntimeError(f"Impossible de g√©n√©rer l'embedding pour le texte: {str(e)}")
    
    def generate_embeddings_batch(self, texts: List[str]) -> np.ndarray:
        """
        G√©n√®re des embeddings pour une liste de textes (traitement par batch pour de meilleures performances).
        
        Args:
            texts (List[str]): Liste de textes √† encoder
            
        Returns:
            np.ndarray: Array des embeddings (shape: [n_texts, embedding_dim])
        """
        if not texts:
            raise ValueError("La liste de textes est vide.")
        
        # Nettoyage de tous les textes
        cleaned_texts = [self._clean_text(text) for text in texts]
        
        # V√©rification que tous les textes ne sont pas vides
        if all(not text.strip() for text in cleaned_texts):
            raise ValueError("Tous les textes sont vides apr√®s nettoyage.")
        
        try:
            # G√©n√©ration des embeddings par batch
            embeddings = self.model.encode(
                cleaned_texts,
                convert_to_numpy=True,
                normalize_embeddings=True,
                batch_size=32,  # Ajustable selon la m√©moire disponible
                show_progress_bar=len(texts) > 10  # Progress bar pour les gros batches
            )
            
            return embeddings
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration des embeddings par batch: {e}")
            raise RuntimeError(f"Impossible de g√©n√©rer les embeddings: {str(e)}")
    
    def compute_similarity(self, text1: str, text2: str) -> float:
        """
        Calcule la similarit√© cosinus entre deux textes.
        
        Args:
            text1 (str): Premier texte
            text2 (str): Deuxi√®me texte
            
        Returns:
            float: Score de similarit√© entre -1 et 1
        """
        try:
            # G√©n√©ration des embeddings pour les deux textes
            embeddings = self.generate_embeddings_batch([text1, text2])
            
            # Calcul de la similarit√© cosinus
            # (Les embeddings sont d√©j√† normalis√©s, donc le produit scalaire = similarit√© cosinus)
            similarity = np.dot(embeddings[0], embeddings[1])
            
            return float(similarity)
            
        except Exception as e:
            print(f"‚ùå Erreur lors du calcul de similarit√©: {e}")
            raise RuntimeError(f"Impossible de calculer la similarit√©: {str(e)}")
    
    def compute_similarity_matrix(self, texts: List[str]) -> np.ndarray:
        """
        Calcule la matrice de similarit√© pour une liste de textes.
        
        Args:
            texts (List[str]): Liste de textes
            
        Returns:
            np.ndarray: Matrice de similarit√© (shape: [n_texts, n_texts])
        """
        if len(texts) < 2:
            raise ValueError("Il faut au moins 2 textes pour calculer une matrice de similarit√©.")
        
        try:
            # G√©n√©ration de tous les embeddings
            embeddings = self.generate_embeddings_batch(texts)
            
            # Calcul de la matrice de similarit√© (produit matriciel)
            similarity_matrix = np.dot(embeddings, embeddings.T)
            
            return similarity_matrix
            
        except Exception as e:
            print(f"‚ùå Erreur lors du calcul de la matrice de similarit√©: {e}")
            raise RuntimeError(f"Impossible de calculer la matrice de similarit√©: {str(e)}")
    
    def get_model_info(self) -> dict:
        """
        Retourne des informations sur le mod√®le utilis√©.
        
        Returns:
            dict: Informations sur le mod√®le
        """
        return {
            "model_name": self.model._modules['0'].auto_model.config.name_or_path,
            "embedding_dimension": self.model.get_sentence_embedding_dimension(),
            "max_sequence_length": self.model.max_seq_length,
            "device": str(self.device)
        }


# Test local du module
if __name__ == "__main__":
    import time
    
    print("üß™ Test du gestionnaire d'embeddings avec Sentence Transformers")
    
    # Initialisation du gestionnaire
    print("\nüì• Chargement du mod√®le d'embeddings...")
    manager = EmbeddingGenerator()
    
    # Informations sur le mod√®le
    info = manager.get_model_info()
    print("\nüìä Informations sur le mod√®le:")
    for key, value in info.items():
        print(f"  ‚Ä¢ {key}: {value}")
    
    # Test de g√©n√©ration d'embeddings individuels
    test_texts = [
        "Avancer √† pleine vitesse",
        "Tourne √† droite rapidement", 
        "Allume les LED en bleu",
        "Recule en arri√®re"
    ]
    
    print("\nüî¢ Test de g√©n√©ration d'embeddings individuels:")
    embeddings_individual = []
    
    for text in test_texts:
        print(f"\n  ‚Ä¢ Texte: '{text}'")
        
        # Mesure du temps d'ex√©cution
        start_time = time.time()
        embedding = manager.generate_embedding(text)
        end_time = time.time()
        duration = end_time - start_time
        
        embeddings_individual.append(embedding)
        
        print(f"    Dimension: {embedding.shape}")
        print(f"    Norme: {np.linalg.norm(embedding):.4f}")
        print(f"    Premiers √©l√©ments: {embedding[:3]}...")
        print(f"    Temps: {duration:.3f}s")
    
    # Test de g√©n√©ration par lot (plus efficace)
    print("\nüöÄ Test de g√©n√©ration d'embeddings par lot:")
    start_time = time.time()
    batch_embeddings = manager.generate_embeddings_batch(test_texts)
    end_time = time.time()
    batch_duration = end_time - start_time
    
    print(f"  ‚Ä¢ Nombre d'embeddings g√©n√©r√©s: {len(batch_embeddings)}")
    print(f"  ‚Ä¢ Forme du batch: {batch_embeddings.shape}")
    print(f"  ‚Ä¢ Temps total pour le batch: {batch_duration:.3f}s")
    print(f"  ‚Ä¢ Temps moyen par embedding: {batch_duration/len(test_texts):.3f}s")
    
    # Test de similarit√© avec la m√©thode int√©gr√©e
    print("\nüîç Test de similarit√© entre textes (m√©thode int√©gr√©e):")
    for i in range(len(test_texts)):
        for j in range(i+1, len(test_texts)):
            similarity = manager.compute_similarity(test_texts[i], test_texts[j])
            print(f"  ‚Ä¢ '{test_texts[i]}' vs '{test_texts[j]}': {similarity:.4f}")
    
    # Test de la matrice de similarit√© compl√®te
    print("\nüîç Test de matrice de similarit√© compl√®te:")
    similarity_matrix = manager.compute_similarity_matrix(test_texts)
    print(f"  ‚Ä¢ Forme de la matrice: {similarity_matrix.shape}")
    print("  ‚Ä¢ Matrice de similarit√©:")
    
    # Affichage format√© de la matrice
    print("    ", end="")
    for i, text in enumerate(test_texts):
        print(f"{i:>8}", end="")
    print()
    
    for i, text in enumerate(test_texts):
        print(f"{i}: ", end="")
        for j in range(len(test_texts)):
            print(f"{similarity_matrix[i,j]:>8.4f}", end="")
        print(f"  ({text[:25]}...)" if len(text) > 25 else f"  ({text})")
    

    
    # Test avec des phrases plus vari√©es pour v√©rifier la discrimination
    print("\nüéØ Test de discrimination avec des phrases plus vari√©es:")
    varied_texts = [
        "Avance tout droit rapidement", 
        "Marche lentement vers l'avant",
        "√âteint toutes les lumi√®res",
        "Calculer la racine carr√©e de 64"  # Hors sujet
    ]
    
    reference_text = "Avancer √† pleine vitesse"
    print(f"  R√©f√©rence: '{reference_text}'")
    
    for text in varied_texts:
        similarity = manager.compute_similarity(reference_text, text)
        print(f"  ‚Ä¢ vs '{text}': {similarity:.4f}")
    
    print("\n‚úÖ Test termin√©! ")