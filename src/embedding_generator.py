import numpy as np
import torch
from sentence_transformers import SentenceTransformer
import re
from typing import List, Union


class EmbeddingGenerator:
    """
    GÃ©nÃ¨re des embeddings en utilisant un modÃ¨le Sentence Transformers multilingue
    spÃ©cialement conÃ§u pour la similaritÃ© sÃ©mantique.
    """
    
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        Initialise le gestionnaire d'embeddings.
        
        Args:
            model_name (str): Nom du modÃ¨le Sentence Transformers.
                             Par dÃ©faut: "paraphrase-multilingual-MiniLM-L12-v2"
        """
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ Utilisation du pÃ©riphÃ©rique : {self.device}")
        
        try:
            # Chargement du modÃ¨le Sentence Transformers
            print(f"ğŸ“¥ Chargement du modÃ¨le {model_name}...")
            self.model = SentenceTransformer(model_name, 
                                             device=str(self.device),
                                             backend="torch")
            
            print("âœ… ModÃ¨le Sentence Transformers chargÃ© et configurÃ©.")
            
        except Exception as e:
            print(f"âŒ Erreur lors du chargement du modÃ¨le: {e}")
            raise RuntimeError(f"Impossible de charger le modÃ¨le {model_name}: {str(e)}")
    
    def _clean_text(self, text: str) -> str:
        """
        Nettoie et normalise le texte d'entrÃ©e.
        
        Args:
            text (str): Texte Ã  nettoyer
            
        Returns:
            str: Texte nettoyÃ©
        """
        if not isinstance(text, str):
            text = str(text)
        
        # Suppression des caractÃ¨res spÃ©ciaux excessifs
        text = re.sub(r'[^\w\sÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¶Ã¹Ã»Ã¼Ã¿Ã§\'-]', ' ', text)
        
        # Normalisation des espaces
        text = re.sub(r'\s+', ' ', text)
        
        # Suppression des espaces en dÃ©but/fin
        text = text.strip()
        
        return text
    
    def generate_embedding(self, text: str) -> np.ndarray:
        """
        GÃ©nÃ¨re un embedding pour un texte donnÃ©.
        
        Args:
            text (str): Texte Ã  encoder
            
        Returns:
            np.ndarray: Embedding du texte (vecteur de features normalisÃ©)
        """
        # Nettoyage et prÃ©paration du texte
        cleaned_text = self._clean_text(text)
        
        if not cleaned_text.strip():
            raise ValueError("Le texte d'entrÃ©e est vide aprÃ¨s nettoyage.")
        
        try:
            # GÃ©nÃ©ration de l'embedding avec Sentence Transformers
            # Le modÃ¨le gÃ¨re automatiquement la tokenisation, l'encodage et la normalisation
            embedding = self.model.encode(
                cleaned_text,
                convert_to_numpy=True,
                normalize_embeddings=True  # Normalisation automatique
            )
            
            return embedding
            
        except Exception as e:
            print(f"âŒ Erreur lors de la gÃ©nÃ©ration de l'embedding: {e}")
            raise RuntimeError(f"Impossible de gÃ©nÃ©rer l'embedding pour le texte: {str(e)}")
    
    def generate_embeddings_batch(self, texts: List[str]) -> np.ndarray:
        """
        GÃ©nÃ¨re des embeddings pour une liste de textes (traitement par batch pour de meilleures performances).
        
        Args:
            texts (List[str]): Liste de textes Ã  encoder
            
        Returns:
            np.ndarray: Array des embeddings (shape: [n_texts, embedding_dim])
        """
        if not texts:
            raise ValueError("La liste de textes est vide.")
        
        # Nettoyage de tous les textes
        cleaned_texts = [self._clean_text(text) for text in texts]
        
        # VÃ©rification que tous les textes ne sont pas vides
        if all(not text.strip() for text in cleaned_texts):
            raise ValueError("Tous les textes sont vides aprÃ¨s nettoyage.")
        
        try:
            # GÃ©nÃ©ration des embeddings par batch
            embeddings = self.model.encode(
                cleaned_texts,
                convert_to_numpy=True,
                normalize_embeddings=True,
                batch_size=32,  # Ajustable selon la mÃ©moire disponible
                show_progress_bar=len(texts) > 10  # Progress bar pour les gros batches
            )
            
            return embeddings
            
        except Exception as e:
            print(f"âŒ Erreur lors de la gÃ©nÃ©ration des embeddings par batch: {e}")
            raise RuntimeError(f"Impossible de gÃ©nÃ©rer les embeddings: {str(e)}")
    
    def compute_similarity(self, text1: str, text2: str) -> float:
        """
        Calcule la similaritÃ© cosinus entre deux textes.
        
        Args:
            text1 (str): Premier texte
            text2 (str): DeuxiÃ¨me texte
            
        Returns:
            float: Score de similaritÃ© entre -1 et 1
        """
        try:
            # GÃ©nÃ©ration des embeddings pour les deux textes
            embeddings = self.generate_embeddings_batch([text1, text2])
            
            # Calcul de la similaritÃ© cosinus
            # (Les embeddings sont dÃ©jÃ  normalisÃ©s, donc le produit scalaire = similaritÃ© cosinus)
            similarity = np.dot(embeddings[0], embeddings[1])
            
            return float(similarity)
            
        except Exception as e:
            print(f"âŒ Erreur lors du calcul de similaritÃ©: {e}")
            raise RuntimeError(f"Impossible de calculer la similaritÃ©: {str(e)}")
    
    def compute_similarity_matrix(self, texts: List[str]) -> np.ndarray:
        """
        Calcule la matrice de similaritÃ© pour une liste de textes.
        
        Args:
            texts (List[str]): Liste de textes
            
        Returns:
            np.ndarray: Matrice de similaritÃ© (shape: [n_texts, n_texts])
        """
        if len(texts) < 2:
            raise ValueError("Il faut au moins 2 textes pour calculer une matrice de similaritÃ©.")
        
        try:
            # GÃ©nÃ©ration de tous les embeddings
            embeddings = self.generate_embeddings_batch(texts)
            
            # Calcul de la matrice de similaritÃ© (produit matriciel)
            similarity_matrix = np.dot(embeddings, embeddings.T)
            
            return similarity_matrix
            
        except Exception as e:
            print(f"âŒ Erreur lors du calcul de la matrice de similaritÃ©: {e}")
            raise RuntimeError(f"Impossible de calculer la matrice de similaritÃ©: {str(e)}")
    
    def get_model_info(self) -> dict:
        """
        Retourne des informations sur le modÃ¨le utilisÃ©.
        
        Returns:
            dict: Informations sur le modÃ¨le
        """
        return {
            "model_name": self.model._modules['0'].auto_model.config.name_or_path,
            "embedding_dimension": self.model.get_sentence_embedding_dimension(),
            "max_sequence_length": self.model.max_seq_length,
            "device": str(self.device)
        }


# Test local du module
if __name__ == "__main__":
    import time
    
    print("ğŸ§ª Test du gestionnaire d'embeddings avec Sentence Transformers")
    
    # Initialisation du gestionnaire
    print("\nğŸ“¥ Chargement du modÃ¨le d'embeddings...")
    manager = EmbeddingGenerator()
    
    # Informations sur le modÃ¨le
    info = manager.get_model_info()
    print("\nğŸ“Š Informations sur le modÃ¨le:")
    for key, value in info.items():
        print(f"  â€¢ {key}: {value}")
    
    # Test de gÃ©nÃ©ration d'embeddings individuels
    test_texts = [
        "Avancer Ã  pleine vitesse",
        "Tourne Ã  droite rapidement", 
        "Allume les LED en bleu",
        "Recule en arriÃ¨re"
    ]
    
    print("\nğŸ”¢ Test de gÃ©nÃ©ration d'embeddings individuels:")
    embeddings_individual = []
    
    for text in test_texts:
        print(f"\n  â€¢ Texte: '{text}'")
        
        # Mesure du temps d'exÃ©cution
        start_time = time.time()
        embedding = manager.generate_embedding(text)
        end_time = time.time()
        duration = end_time - start_time
        
        embeddings_individual.append(embedding)
        
        print(f"    Dimension: {embedding.shape}")
        print(f"    Norme: {np.linalg.norm(embedding):.4f}")
        print(f"    Premiers Ã©lÃ©ments: {embedding[:3]}...")
        print(f"    Temps: {duration:.3f}s")
    
    # Test de gÃ©nÃ©ration par lot (plus efficace)
    print("\nğŸš€ Test de gÃ©nÃ©ration d'embeddings par lot:")
    start_time = time.time()
    batch_embeddings = manager.generate_embeddings_batch(test_texts)
    end_time = time.time()
    batch_duration = end_time - start_time
    
    print(f"  â€¢ Nombre d'embeddings gÃ©nÃ©rÃ©s: {len(batch_embeddings)}")
    print(f"  â€¢ Forme du batch: {batch_embeddings.shape}")
    print(f"  â€¢ Temps total pour le batch: {batch_duration:.3f}s")
    print(f"  â€¢ Temps moyen par embedding: {batch_duration/len(test_texts):.3f}s")
    
    # Test de similaritÃ© avec la mÃ©thode intÃ©grÃ©e
    print("\nğŸ” Test de similaritÃ© entre textes (mÃ©thode intÃ©grÃ©e):")
    for i in range(len(test_texts)):
        for j in range(i+1, len(test_texts)):
            similarity = manager.compute_similarity(test_texts[i], test_texts[j])
            print(f"  â€¢ '{test_texts[i]}' vs '{test_texts[j]}': {similarity:.4f}")
    
    # Test de la matrice de similaritÃ© complÃ¨te
    print("\nğŸ” Test de matrice de similaritÃ© complÃ¨te:")
    similarity_matrix = manager.compute_similarity_matrix(test_texts)
    print(f"  â€¢ Forme de la matrice: {similarity_matrix.shape}")
    print("  â€¢ Matrice de similaritÃ©:")
    
    # Affichage formatÃ© de la matrice
    print("    ", end="")
    for i, text in enumerate(test_texts):
        print(f"{i:>8}", end="")
    print()
    
    for i, text in enumerate(test_texts):
        print(f"{i}: ", end="")
        for j in range(len(test_texts)):
            print(f"{similarity_matrix[i,j]:>8.4f}", end="")
        print(f"  ({text[:25]}...)" if len(text) > 25 else f"  ({text})")
    

    
    # Test avec des phrases plus variÃ©es pour vÃ©rifier la discrimination
    print("\nğŸ¯ Test de discrimination avec des phrases plus variÃ©es:")
    varied_texts = [
        "Avance tout droit rapidement", 
        "Marche lentement vers l'avant",
        "Ã‰teint toutes les lumiÃ¨res",
        "Calculer la racine carrÃ©e de 64"  # Hors sujet
    ]
    
    reference_text = "Avancer Ã  pleine vitesse"
    print(f"  RÃ©fÃ©rence: '{reference_text}'")
    
    for text in varied_texts:
        similarity = manager.compute_similarity(reference_text, text)
        print(f"  â€¢ vs '{text}': {similarity:.4f}")
    
    print("\nâœ… Test terminÃ©! ")