"""
Gestionnaire d'embeddings utilisant BERT fran√ßais pour VoxThymio
Permet de g√©n√©rer des embeddings de descriptions en langage naturel
et d'effectuer des recherches de similarit√©.

D√©velopp√© par Esp√©rance AYIWAHOUN pour AI4Innov
"""

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
from typing import List, Tuple, Dict, Any
import warnings

# Suppression des avertissements
warnings.filterwarnings("ignore", category=FutureWarning)


class EmbeddingManager:
    """
    Gestionnaire d'embeddings utilisant un mod√®le BERT fran√ßais.
    G√©n√®re des embeddings √† partir de descriptions en langage naturel.
    """
    
    def __init__(self, model_name: str = "camembert-base"):
        """
        Initialise le gestionnaire d'embeddings.
        
        Args:
            model_name (str): Nom du mod√®le BERT fran√ßais √† utiliser.
                             Par d√©faut: "camembert-base" (BERT fran√ßais)
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üîß Utilisation du p√©riph√©rique : {self.device}")
        
        try:
            # Chargement du mod√®le et tokenizer BERT fran√ßais
            print(f"üì• Chargement du mod√®le {model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name)
            
            # Transfert sur le bon p√©riph√©rique
            self.model.to(self.device)
            self.model.eval()
            
            print("‚úÖ Mod√®le BERT fran√ßais charg√© et configur√©.")
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            raise RuntimeError(f"Impossible de charger le mod√®le {model_name}: {str(e)}")
    
    def generate_embedding(self, text: str) -> np.ndarray:
        """
        G√©n√®re un embedding pour un texte donn√©.
        
        Args:
            text (str): Texte √† encoder
            
        Returns:
            np.ndarray: Embedding du texte (vecteur de features)
        """
        # Nettoyage et pr√©paration du texte
        cleaned_text = self._clean_text(text)
        
        # Tokenisation
        inputs = self.tokenizer(
            cleaned_text,
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=128
        )
        
        # D√©placement sur le bon p√©riph√©rique
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # G√©n√©ration de l'embedding
        with torch.no_grad():
            outputs = self.model(**inputs)
            # Utilisation du token [CLS] (premier token) comme repr√©sentation
            embedding = outputs.last_hidden_state[0][0].cpu().numpy()
        
        return embedding
    
    def generate_embeddings_batch(self, texts: List[str]) -> List[np.ndarray]:
        """
        G√©n√®re des embeddings pour une liste de textes.
        
        Args:
            texts (List[str]): Liste de textes √† encoder
            
        Returns:
            List[np.ndarray]: Liste des embeddings
        """
        embeddings = []
        for text in texts:
            embedding = self.generate_embedding(text)
            embeddings.append(embedding)
        return embeddings
    
    def calculate_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """
        Calcule la similarit√© cosinus entre deux embeddings.
        
        Args:
            embedding1 (np.ndarray): Premier embedding
            embedding2 (np.ndarray): Deuxi√®me embedding
            
        Returns:
            float: Score de similarit√© entre 0 et 1
        """
        # Calcul de la similarit√© cosinus
        dot_product = np.dot(embedding1, embedding2)
        norm1 = np.linalg.norm(embedding1)
        norm2 = np.linalg.norm(embedding2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = dot_product / (norm1 * norm2)
        # Normalisation entre 0 et 1
        return (similarity + 1) / 2
    
    def find_most_similar(self, query_embedding: np.ndarray, 
                         embeddings: List[np.ndarray], 
                         threshold: float = 0.6) -> Tuple[int, float]:
        """
        Trouve l'embedding le plus similaire √† une requ√™te.
        
        Args:
            query_embedding (np.ndarray): Embedding de la requ√™te
            embeddings (List[np.ndarray]): Liste des embeddings de r√©f√©rence
            threshold (float): Seuil de similarit√© minimum
            
        Returns:
            Tuple[int, float]: Index de l'embedding le plus similaire et son score
                              (-1, 0.0) si aucun ne d√©passe le seuil
        """
        if not embeddings:
            return -1, 0.0
        
        max_similarity = 0.0
        best_index = -1
        
        for i, embedding in enumerate(embeddings):
            similarity = self.calculate_similarity(query_embedding, embedding)
            if similarity > max_similarity and similarity >= threshold:
                max_similarity = similarity
                best_index = i
        
        return best_index, max_similarity
    
    def _clean_text(self, text: str) -> str:
        """
        Nettoie et normalise le texte d'entr√©e.
        
        Args:
            text (str): Texte √† nettoyer
            
        Returns:
            str: Texte nettoy√©
        """
        if not text:
            return ""
        
        # Conversion en minuscules et suppression des espaces superflus
        cleaned = text.lower().strip()
        # Suppression des caract√®res de contr√¥le et normalisation des espaces
        cleaned = ' '.join(cleaned.split())
        
        return cleaned
    
    def get_embedding_info(self) -> Dict[str, Any]:
        """
        Retourne des informations sur le mod√®le d'embedding.
        
        Returns:
            Dict[str, Any]: Informations sur le mod√®le
        """
        return {
            "model_name": self.model.config.name_or_path if hasattr(self.model, 'config') else "Unknown",
            "embedding_dim": self.model.config.hidden_size if hasattr(self.model, 'config') else "Unknown",
            "device": str(self.device),
            "vocab_size": len(self.tokenizer) if self.tokenizer else "Unknown"
        }
